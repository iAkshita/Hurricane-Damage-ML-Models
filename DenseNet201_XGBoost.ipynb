{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":742001,"sourceType":"datasetVersion","datasetId":383256}],"dockerImageVersionId":30408,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing essential libraries\nimport os  # Operating system related functions\nimport numpy as nu  # Library for numerical computing \nimport pandas as ps  # Library for manipulation \nimport seaborn as sb  # Library of visualizating the data\nimport tensorflow as tnf  # framework deep learning \nimport itertools  # Utility functions to iterate\nfrom tensorflow.keras.applications.densenet import DenseNet201  # Importing pre-trained model\nfrom tensorflow.keras.preprocessing import image  #  Preprocessing image\nfrom tensorflow.keras.applications.densenet import preprocess_input  # Preprocessing function for DenseNet201\nfrom tensorflow.keras.layers import Dense, Dropout  # Neural network\nfrom tensorflow.keras.models import Sequential  # Sequential neural network model\nfrom tensorflow.keras.regularizers import l2  # Regularization l2\nfrom keras.utils.vis_utils import plot_model  # Visualization of Model \nfrom sklearn.model_selection import train_test_split  # Splitting the dataset\nfrom sklearn.metrics import classification_report  # classification report importing \nfrom sklearn.metrics import confusion_matrix  # Confusion matrix importing\nfrom sklearn.metrics import matthews_corrcoef as MTC  # Matthews correlation coefficient importing\nfrom sklearn.metrics import balanced_accuracy_score as BS  # Balanced accuracy score importing\nfrom sklearn.preprocessing import OneHotEncoder  # One-hot encoding\nfrom sklearn.utils import shuffle  # Shuffling data\nimport tensorflow_addons as tnfa  # Additional TensorFlow addons\nimport matplotlib.pyplot as mplt  # Plotting library\nimport matplotlib.gridspec as gridspec  # Subplots grid specification \nfrom distutils.dir_util import copy_tree, remove_tree  # Directory manipulation\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator as IDGN  # Importing image data generator for data augmentation","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a list of class labels, in this case, \"damage\" and \"no_damage\"\nh_class = [\"damage\", \"no_damage\"]\n\n# Define the base path to a directory containing satellite images of hurricane damage\nh_pth = \"/kaggle/input/satellite-images-of-hurricane-damage/train_another\"\n\n# Create a list of file paths by joining the base path with specific image file names\nh_fpth = [\n    os.path.join(h_pth, \"damage/-93.55964_30.895018.jpeg\"),  # File path for a damaged image\n    os.path.join(h_pth, \"no_damage/-95.061275_29.831535.jpeg\")  # File path for an undamaged image\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a DenseNet201 model of input shape for processing images.\n# include_top=True means that the model includes its fully connected top layer.\nh_modl = DenseNet201(input_shape=(224, 224, 3), include_top=True)\n\n# Define a list of metrics to be used for model evaluation.\nh_metri = [\n    tnf.keras.metrics.CategoricalAccuracy(name='acc'),  # Categorical accuracy metric\n    tnf.keras.metrics.AUC(name='auc'),  # Area Under the ROC Curve metric\n    tnfa.metrics.F1Score(num_classes=2)  # F1 Score metric for a binary classification problem\n]\n\n# Compiling model using optimizer Adam,\n# loss function of categorical cross-entropy, specified metrics.\nh_modl.compile(optimizer='adam',\n              loss=tnf.losses.CategoricalCrossentropy(),\n              metrics=h_metri)\n\n# Display a summary of the model architecture, including layers and parameter counts.\nh_modl.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the output tensor of the \"avg_pool\" layer from the pre-trained model (h_modl)\nvector = h_modl.get_layer(\"avg_pool\").output\n\n# Create a feature extractor model by specifying the input tensor (h_modl.input)\n# and the output tensor (vector)\nfeature_extractor = tnf.keras.Model(h_modl.input, vector)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create empty lists to store feature vectors (P_list) and corresponding labels (Q_list)\nP_list = []\nQ_list = []\n\n# Loop through two classes (0: \"damage\" and 1: \"no_damage\")\nfor f in range(2):    \n    # Construct the path to the class-specific directory\n    h_folder_path = os.path.join(h_pth, h_class[f])\n    \n    # Iterate through files in the class-specific directory\n    for h_phile in os.listdir(h_folder_path):    \n        # Build the full file path\n        h_fpath = os.path.join(h_folder_path, h_phile)\n        \n        # Check if the file has a \".jpeg\" extension; skip it if not\n        if not h_phile.endswith(\".jpeg\"):\n            continue\n        \n        # Load the image with a target size of (224, 224)\n        h_imj = image.load_img(h_fpath, target_size=(224, 224))\n        \n        # Converting  image\n        # Image converted to array\n        h_imj_arr = image.img_to_array(h_imj)\n        \n        # Adding one more dimensions to the image array\n        imj_arr_b = nu.expand_dims(h_imj_arr, axis=0)\n        \n        # Preprocess the image\n        h_input_imj = preprocess_input(imj_arr_b)\n        \n        # Extract features using a pre-trained feature extractor (assuming 'feature_extractor' is defined elsewhere)\n        feature_vec = feature_extractor.predict(h_input_imj)\n    \n        # Append the flattened feature vector to P_list\n        P_list.append(feature_vec.ravel())\n        \n        # Append the class label (0 or 1) to Q_list\n        Q_list.append(f)\n\n# Convert the Python lists to NumPy arrays with data type float32\nP_list = nu.asarray(P_list, dtype=nu.float32)\nQ_list = nu.asarray(Q_list, dtype=nu.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert the Python lists P_list and Q_list to NumPy arrays with data type float32\nP = nu.asarray(P_list, dtype=nu.float32)\nQ = nu.asarray(Q_list, dtype=nu.float32)\n\n# Shuffle the data (P and Q) 100 times to randomize the order of samples\nfor s in range(100):\n    P, Q = shuffle(P, Q)\n\n# Print the shapes of the feature matrix P and label matrix Q\nprint(\"The shapes of feature matrix P\") # printing matrix p\nprint(P.shape)\nprint(\"\\nThe shapes of label matrix Q\") # printing matrix q\nprint(Q.shape)\n\n# Find unique class labels and their corresponding counts in the label matrix Q\nclass_types, conts = nu.unique(Q, return_counts=True)\n\n# Print the unique class labels and their counts\nprint(\"\\nClass labels\") # printing labels\nprint(class_types)      # pronting class types\nprint(\"\\nClass counts\") # printing counts\nprint(conts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the feature matrix P and label matrix Q \n# In sets of testing and training \n# 'test_size' parameter specifing data proportion used for testing (20%, under this case)\n# 'stratify=Q' ensures that the class distribution in the training and testing sets is similar to that in Q\n# 'random_state=0' sets a seed for the random number generator to ensure reproducibility\ntrain_P, test_P, train_Q, test_Q = train_test_split(P, \n                                                  Q, \n                                                  test_size=0.2,\n                                                  stratify=Q,\n                                                  random_state=0)\n\n# Printing shapes of sets belongs to testing and training\nprint(\"train_P  is\") # shape of train_P\nprint(train_P.shape)\nprint(\"\\ntest_P  is\")# shape of test_P\nprint(test_P.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import the necessary libraries for XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# Create an XGBoost classifier with specified hyperparameters (C=1.75 and kernel=\"linear\")\nh_xgb_lin = xgb.XGBClassifier(C=1.75, kernel=\"linear\")\n\n# Fitting XGBoost classifier to training data\nh_xgb_lin.fit(train_P, train_Q)\n\n# Making predictions on test data using trained model\nq_pred = h_xgb_lin.predict(test_P)\n\n# Printing report of classification, \n# Provideing metrics belongs to evaluation\n# like recall,precision,and F1-score\n# 'target_names' parameter is used to label the classes in report\nprint(classification_report(test_Q, q_pred, target_names=h_class))\n\n# Print a confusion matrix to visualize the model's performance\nprint(confusion_matrix(test_Q, q_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def the_matrix(value,\n               the_name,\n               title='Confusion Matrix',\n               cmap=None,\n               normalize=True):\n    \"\"\"\n    Plot a confusion matrix.\n\n    Parameters:\n    - value: The confusion matrix values as a numpy array.\n    - the_name: Labels for the classes.\n    - title: The title for the plot (default is 'Confusion Matrix').\n    - cmap: Colormap for the plot (default is None, which uses 'Blues').\n    - normalize: Whether to normalize the confusion matrix (default is True).\n\n    Returns:\n    - None\n    \"\"\"\n\n    # Calculate accuracy and misclassification rate\n    acrcy = nu.trace(value) / float(nu.sum(value))\n    mis = 1 - acrcy\n\n    # Set the colormap to 'Blues' if not specified\n    if cmap is None:\n        cmap = mplt.get_cmap('Blues')\n\n    # Create a figure for the plot with specified size\n    mplt.figure(figsize=(8, 6))\n\n    # Plotting confusion matrix as image\n    mplt.imshow(value, interpolation='nearest', cmap=cmap)\n\n    # Setting title of plot\n    mplt.title(title)\n\n    # Add color bar to  plot\n    mplt.colorbar()\n\n    # Customize x and y axis labels if class labels are provided\n    if the_name is not None:\n        the_marking = nu.arange(len(the_name))\n        mplt.xticks(the_marking, the_name, rotation=90)\n        mplt.yticks(the_marking, the_name)\n\n    # Normalize the confusion matrix if specified\n    if normalize:\n        value = value.astype('float') / value.sum(axis=1)[:, nu.newaxis]\n\n    # Set the threshold for text color based on normalization\n    h_boundary = value.max() / 1.5 if normalize else value.max() / 2\n\n    # Add text annotations to the plot\n    for s, t in itertools.product(range(value.shape[0]), range(value.shape[1])):\n        if normalize:\n            mplt.text(t, s, \"{:0.4f}\".format(value[s, t]),\n                      horizontalalignment=\"center\",\n                      color=\"white\" if value[s, t] > h_boundary else \"black\")\n        else:\n            mplt.text(t, s, \"{:,}\".format(value[s, t]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if value[s, t] > h_boundary else \"black\")\n\n    # Adjust the layout of the plot for better appearance\n    mplt.tight_layout()\n\n    # Set labels for y-axis and x-axis\n    mplt.ylabel('ACTUAL', fontsize=20)\n    mplt.xlabel('PREDICTED', fontsize=14)\n\n    # Show the plot\n    mplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a dictionary 'font' to set the font properties\nfont = {'family': 'serif',  # Font family (serif)\n        'weight': 'bold',   # Font weight (bold)\n        'size': 20}         # Font size (20 points)\n\n# Use 'mplt.rc' to set the global font properties using the 'font' dictionary\nmplt.rc('font', **font)\n\n# Set the font family to \"serif\" for all plots\nmplt.rcParams[\"font.family\"] = \"serif\"\n\n# Set the default font size for all plots to 14 points\nmplt.rcParams[\"font.size\"] = 14","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the 'the_matrix' function to plot a confusion matrix with the provided arguments\nthe_matrix(value=nu.array([[966, 34], [26, 974]]),  # Confusion matrix values\n           normalize=False,                         # Not normalizing the values\n           the_name=[\"Damage\", \"NonDamage\"],        # Class labels\n           title=\"XGBoost\")                         # Title for the plot\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import the OneHotEncoder class from the appropriate library (not shown in the provided code)\nn_encoder = OneHotEncoder(sparse=False)\n\n# Fit the encoder to the training labels (train_Y), reshaping them to a single column\nn_encoder.fit(train_Q.reshape(-1, 1))\n\n# Transform the training labels (train_Q) into one-hot encoded format\ne_train_Q = n_encoder.transform(train_Q.reshape(-1, 1))\n\n# Transform the testing labels (test_Q) into one-hot encoded format\ne_test_Q = n_encoder.transform(test_Q.reshape(-1, 1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a function for creating model neural network \ndef create_model():\n    h_modl = Sequential()\n    # Adding dense layer with 1000 units, input dimension of 1920, and ReLU activation\n    h_modl.add(Dense(1000, input_dim=1920, activation=\"relu\"))\n    # Adding layer dropout regularization with a rate of 0.3\n    h_modl.add(Dropout(0.3))\n    # Adding another dense layer with 2 units, L2 regularization of 0.1, and linear activation\n    h_modl.add(Dense(2, kernel_regularizer=l2(0.1), activation=\"linear\"))\n    # Compiling model with optimizer Adam , specified metrics, hinge loss categorical\n    h_modl.compile(optimizer=tnf.keras.optimizers.Adam(lr=0.0001),\n                   loss=\"categorical_hinge\", metrics=h_metri)\n    return h_modl\n\n# Defining number of epochs belongs to training\nepoch = 1000\n\n# Creating model using function 'create_model' \nh_modl = create_model()\n\n# Training model on data belongs to training with validation split, specified epochs, batch size, and verbosity\nhistory = h_modl.fit(train_P, e_train_Q,\n                     validation_split=0.15,\n                     epochs=epoch, batch_size=64, verbose=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure with 3 subplots in a 1x3 grid with a specified figsize\nfig, aex = mplt.subplots(1, 3, figsize=(20, 10))\naex = aex.ravel()\n\n# Iterate over the metrics [\"acc\", \"auc\", \"loss\"]\nfor s, the_metri in enumerate([\"acc\", \"auc\", \"loss\"]):\n    # Plot the training and validation metrics for each metric\n    aex[s].plot(history.history[the_metri])\n    aex[s].plot(history.history[\"val_\" + the_metri])\n    \n    # Setting title, y-axis and x-axis label,\n    # label for each subplot\n    aex[s].set_title(\"Model {}\".format(the_metri))\n    aex[s].set_xlabel(\"Epochs\")\n    aex[s].set_ylabel(the_metri)\n    \n    # Adding legends to differentiate between training and validation data\n    aex[s].legend([\"train\", \"val\"])\n    \n# Define font properties for the plots\nfont = {'family': 'serif',  # Font family (serif)\n        'weight': 'bold',    # Font weight (bold)\n        'size': 12}          # Font size (12 points)\n\n# Setting the specified font properties for the entire plot\nmplt.rc('font', **font)\n\n# Setting the font family to \"serif\" for all plots\nmplt.rcParams[\"font.family\"] = \"serif\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create an array 'e' of evenly spaced values from 1 to 'epoch' with 'epoch' data points\ne = nu.linspace(1, epoch, epoch)\n\n# Create a figure with one subplot using Seaborn (imported as 'sb')\nfig, aexes = mplt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n\n# Plotting training loss over epochs\nsb.lineplot(x=e, y=history.history[\"loss\"], aex=aexes, label=\"train\")\n\n# Plotting validation loss over epochs\nsb.lineplot(x=e, y=history.history[\"val_loss\"], aex=aexes, label=\"val\")\n\n# Setting y-axis label for the plot\naexes.set_ylabel(\"Categorical Hinge Loss\")\n\n# Setting x-axis label for the plot\naexes.set_xlabel(\"Epoch\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predicting labels class for test_P \n# Using trained model and getting class with highest probability\nq_pred = nu.argmax(h_modl.predict(test_P), axis=-1)\n\n# Print a classification report to evaluate the model's performance\n# The 'target_names' parameter is used to label the classes in the report\nprint(classification_report(test_Q, q_pred, targeted_names=h_class))\n\n# Print a confusion matrix to visualize the model's performance\nprint(confusion_matrix(test_Q, q_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a figure with 2 subplots in a 2x1 grid\nfig, aex = mplt.subplots(2, 1)\n\n# Plotting loss belongs to training in the first subplot ('aex[0]') with a blue line and label\naex[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n\n# Plotting loss belongs to testing in the first subplot ('aex[0]') with a red line and label\naex[0].plot(history.history['val_loss'], color='r', label=\"Testing loss\")\n\n# Adding legend to initial subplot for differentiate between loss belongs to testing and training\nlegend = aex[0].legend(loc='best', shadow=True)\n\n# Display the plot\nmplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def the_matrix(value,\n               the_name,\n               title='Confusion matrix',\n               cmap=None,\n               normalize=True):           # defining own confusion matrix function\n\n    # Calculate accuracy and misclassification rate\n    acrcy = nu.trace(value) / float(nu.sum(value))\n    mis = 1 - acrcy\n\n    # Setting colormap to 'Blues' if not specified\n    if cmap is None:\n        cmap = mplt.get_cmap('Blues')\n\n    # Create a figure for the plot with specified size\n    mplt.figure(figsize=(8, 6))\n\n    # Plot the confusion matrix as an image\n    mplt.imshow(value, interpolation='nearest', cmap=cmap)\n\n    # Setting plot's title\n    mplt.title(title)\n\n    # Adding plot's color bar\n    mplt.colorbar()\n\n    # Customize x and y axis labels if class labels are provided\n    if the_name is not None:\n        the_marking = nu.arange(len(the_name))\n        mplt.xticks(the_marking, the_name, rotation=90)\n        mplt.yticks(the_marking, the_name)\n\n    # Normalize the confusion matrix if specified\n    if normalize:\n        value = value.astype('float') / value.sum(axis=1)[:, nu.newaxis]\n\n    # Set the threshold for text color based on normalization\n    h_boundary = value.max() / 1.5 if normalize else value.max() / 2\n\n    # Add text annotations to the plot\n    for s, t in itertools.product(range(value.shape[0]), range(value.shape[1])):\n        if normalize:\n            mplt.text(t, s, \"{:0.4f}\".format(value[s, t]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if value[s, t] > h_boundary else \"black\")\n        else:\n            mplt.text(t, s, \"{:,}\".format(value[s, t]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if value[s, t] > h_boundary else \"black\")\n\n    # Adjust the layout of the plot for better appearance\n    mplt.tight_layout()\n\n    # Set labels for y-axis and x-axis\n    mplt.ylabel('ACTUAL', fontsize=20)\n    # setting font as 20\n    mfont = {'family': 'serif', 'weight': 'bold'}\n    # setting font to be bold\n    mplt.xlabel('PREDICTED', fontsize=14)\n    # predicted font size to be 14\n    font = {'family': 'serif', 'weight': 'bold'}\n    #setting font to be bold\n    # Display the plot\n    mplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a dictionary 'font' to set the font properties\nfont = {'family': 'serif',  # Font family (serif)\n        'weight': 'bold',    # Font weight (bold)\n        'size': 20}          # Font size (20 points)\n\n# Use 'mplt.rc' to set the global font properties using the 'font' dictionary\nmplt.rc('font', **font)\n\n# Set the font family to \"serif\" for all plots\nmplt.rcParams[\"font.family\"] = \"serif\"\n\n# Set the default font size for all plots to 14 points\nmplt.rcParams[\"font.size\"] = 14","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the 'the_matrix' function to plot a confusion matrix with the provided arguments\nthe_matrix(value=nu.array([[976, 24], [28, 972]]),  # Confusion matrix values\n           normalize=False,                         # Not normalizing the values\n           the_name=[\"Damage\", \"NonDamage\"],       # Class labels\n           title=\"DenseNet201_XGBoost\")            # Title for the plot  ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}